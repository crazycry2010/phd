\thusetup{
  %******************************
  % 注意：
  %   1. 配置里面不要出现空行
  %   2. 不需要的配置信息可以删除
  %******************************
  %
  %=====
  % 秘级
  %=====
  secretlevel={秘密},
  secretyear={10},
  %
  %=========
  % 中文信息
  %=========
  ctitle={卷积神经网络：组合卷积结构、随机区域池化和模型压缩},
  cdegree={工学博士},
  cdepartment={计算机科学与技术系},
  cmajor={计算机科学与技术},
  cauthor={王振扬},
  csupervisor={邓志东教授},
  %cassosupervisor={陈文光教授}, % 副指导老师
  %ccosupervisor={某某某教授}, % 联合指导老师
  % 日期自动使用当前时间，若需指定按如下方式修改：
  % cdate={超新星纪元},
  %
  % 博士后专有部分
  %cfirstdiscipline={计算机科学与技术},
  %cseconddiscipline={系统结构},
  %postdoctordate={2009年7月——2011年7月},
  %id={编号}, % 可以留空： id={},
  %udc={UDC}, % 可以留空
  %catalognumber={分类号}, % 可以留空
  %
  %=========
  % 英文信息
  %=========
  etitle={Convolutional Neural Network:  Module Combination Convolution, Stochastic Area Pooling, and Model Compression},
  % 这块比较复杂，需要分情况讨论：
  % 1. 学术型硕士
  %    edegree：必须为Master of Arts或Master of Science（注意大小写）
  %             “哲学、文学、历史学、法学、教育学、艺术学门类，公共管理学科
  %              填写Master of Arts，其它填写Master of Science”
  %    emajor：“获得一级学科授权的学科填写一级学科名称，其它填写二级学科名称”
  % 2. 专业型硕士
  %    edegree：“填写专业学位英文名称全称”
  %    emajor：“工程硕士填写工程领域，其它专业学位不填写此项”
  % 3. 学术型博士
  %    edegree：Doctor of Philosophy（注意大小写）
  %    emajor：“获得一级学科授权的学科填写一级学科名称，其它填写二级学科名称”
  % 4. 专业型博士
  %    edegree：“填写专业学位英文名称全称”
  %    emajor：不填写此项
  edegree={Doctor of Philosophy},
  emajor={Computer Science and Technology},
  eauthor={Wang Zhenyang},
  esupervisor={Professor Deng Zhidong},
  %eassosupervisor={Chen Wenguang},
  % 日期自动生成，若需指定按如下方式修改：
  % edate={December, 2005}
  %
  % 关键词用“英文逗号”分割
  ckeywords={计算机视觉, 深度学习, 卷积神经网络},
  ekeywords={Computer Vision, Deep Learning, Convolutional Neural Networks}
}

% 定义中英文摘要和关键字
\begin{cabstract}
  
 近年来，卷积神经网络在计算机视觉、自然语言处理、语音识别等诸多领域均取得了突破性研究进展。特别是在视觉物体识别任务上，随着标注数据的积累和GPU计算性能的提升，卷积神经网络取得了越来越多的研究成果。本文针对视觉物体识别问题，研究了卷积神经网络在结构和方法上的改进。 取得的主要研究成果包括：
  
  \begin{itemize}
    \item 提出了组合卷积结构来简化复杂网络结构的设计，简化网络设计过程中感受野大小和网络深度选择的难题。组合卷积是一种包含多种网络单元的组合结构，集成了盗梦空间单元Inception、最大化输出单元Maxout、残差网络单元ResNet和网络中的网络NIN 四个卷积模型的优势。组合卷积结构由四条不同的特征提取分支和一个特征选择器构成。其中包括两条具有不同深度和感受野大小的卷积分支，一条残差分支用于加快网络的收敛速度，一条Maxout分支用于增强模型的非线性拟合能力，最后通过一个选择器对特征进行筛选与组合。
    \item 提出了一个由随机区域池化和T型网络结构组成的网络模型。在随机区域池化方法中，采用由仿射变换随机生成的池化区域来代替原来固定不变的池化区域，但是仍然保留了传统的池化操作即最大池化或均值池化。随机区域池化可以看做是对中间层特征的一种重采样方法，起到特征増广的目的。同时我们提出一种通用的网络结构T型结构，可以有效地避免网络设计过程中的特征表达瓶颈问题。
    \item 提出了一种基于主导卷积核分解和知识预回归的模型压缩方法来加快网络的预测时间。主导卷积核分解是一种受矩阵低秩分解启发的卷积参数压缩方法。知识预回归通过将知识从教师网络迁移到压缩后学生网络，让学生网络逐层地学习教师网络的泛化能力，尽量弥补学生网络因参数压缩所造成的精度损失。
   \end{itemize}

 最后，我们以交通标志识别为应用背景，对组合卷积结构和随机区域池化进行了测试与验证。并且将两种方法结合，应用于德国交通标志识别数据集GTSTB上，取得了99.66\%的测试精度。进一步采用主导卷积核分解与知识预回归的方法对模型进行压缩与加速，最终取得了近10倍的模型压缩率和预测速度提升，并且保持了一个较高的识别精度99.27\%。
 
 \end{cabstract}

% 如果习惯关键字跟在摘要文字后面，可以用直接命令来设置，如下：
% \ckeywords{\TeX, \LaTeX, CJK, 模板, 论文}

\begin{eabstract}

Convolutional neural networks play an increasingly important role in computer vision tasks, especially in the field of visual object recognition. In this dissertation, we study on the improvements of convolutional neural networks for visual object recognition. The innovative achievements for this dissertation include:

  \begin{itemize}
    \item We propose a novel module called self-adaptive module (SAM) to simplify design of any new deep learning architecture, since it no longer requires consideration of how to select receptive fields and depths. SAM is a combination of five prominent models, such as Inception, Maxout, ResNet, and NIN. It consists of four passes and one selector. Specifically, the four passes include two direct passes with different receptive fields and depths, one residual pass, and one Maxout pass. Actually, the residual pass is used to speed up convergence, while we take advantage of the Maxout pass to enhance approximate capabilities of SAM. The selector is further designed to help choose reasonable output.
    \item We propose a novel network model that incorporates a stochastic area pooling (SAP) method with a generic stacked T-shaped CNN architecture. In our SAP method, pooling area is randomly transformed and max pooling operation is then conducted on such areas, which means that regular pooling area of fixed upright squares are no longer exploited in the training phase of our SAPNet. In a sense, it could be viewed as the use of feature-level augmentation. Meanwhile, we present a generic CNN architecture that structurally resembles three stacked T-shaped cubes. In such architecture, the number of kernels in convolutional layer preceding any pooling layer is doubled to prevent the representational bottleneck problem.
    \item Aiming at accelerating the test time of deep convolutional neural networks, we propose a model compression method that contains a novel dominant kernel (DK) and a new training method called knowledge pre-regression (KP). DK is presented to significantly accomplish a low-rank decomposition of convolutional kernels, while KP is employed to transfer knowledge of intermediate hidden layers from a larger teacher network to its compressed student network on the basis of a cross entropy loss function instead of previous Euclidean distance.
  \end{itemize}

Finally, we demonstrate the effectiveness of our methods for traffic sign classification. On GTSRB dataset, we achieve a test accuracy of 99.66\%, and compress the number of parameters by a factor about 10. The compressed network is 10 times faster than before, and achieve a test accuracy of 99.27\%.


\end{eabstract}

% \ekeywords{\TeX, \LaTeX, CJK, template, thesis}
