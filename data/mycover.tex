\thusetup{
  %******************************
  % 注意：
  %   1. 配置里面不要出现空行
  %   2. 不需要的配置信息可以删除
  %******************************
  %
  %=====
  % 秘级
  %=====
  secretlevel={秘密},
  secretyear={10},
  %
  %=========
  % 中文信息
  %=========
  ctitle={卷积神经网络：组合卷积结构、随机区域池化和模型压缩},
  cdegree={工学博士},
  cdepartment={计算机科学与技术系},
  cmajor={计算机科学与技术},
  cauthor={王振扬},
  csupervisor={邓志东教授},
  %cassosupervisor={陈文光教授}, % 副指导老师
  %ccosupervisor={某某某教授}, % 联合指导老师
  % 日期自动使用当前时间，若需指定按如下方式修改：
  % cdate={超新星纪元},
  %
  % 博士后专有部分
  %cfirstdiscipline={计算机科学与技术},
  %cseconddiscipline={系统结构},
  %postdoctordate={2009年7月——2011年7月},
  %id={编号}, % 可以留空： id={},
  %udc={UDC}, % 可以留空
  %catalognumber={分类号}, % 可以留空
  %
  %=========
  % 英文信息
  %=========
  etitle={Convolutional Neural Network:  Module Combination Convolution, Stochastic Area Pooling, and Model Compression},
  % 这块比较复杂，需要分情况讨论：
  % 1. 学术型硕士
  %    edegree：必须为Master of Arts或Master of Science（注意大小写）
  %             “哲学、文学、历史学、法学、教育学、艺术学门类，公共管理学科
  %              填写Master of Arts，其它填写Master of Science”
  %    emajor：“获得一级学科授权的学科填写一级学科名称，其它填写二级学科名称”
  % 2. 专业型硕士
  %    edegree：“填写专业学位英文名称全称”
  %    emajor：“工程硕士填写工程领域，其它专业学位不填写此项”
  % 3. 学术型博士
  %    edegree：Doctor of Philosophy（注意大小写）
  %    emajor：“获得一级学科授权的学科填写一级学科名称，其它填写二级学科名称”
  % 4. 专业型博士
  %    edegree：“填写专业学位英文名称全称”
  %    emajor：不填写此项
  edegree={Doctor of Philosophy},
  emajor={Computer Science and Technology},
  eauthor={Wang Zhenyang},
  esupervisor={Professor Deng Zhidong},
  %eassosupervisor={Chen Wenguang},
  % 日期自动生成，若需指定按如下方式修改：
  % edate={December, 2005}
  %
  % 关键词用“英文逗号”分割
  ckeywords={计算机视觉, 深度学习, 卷积神经网络},
  ekeywords={Computer Vision, Deep Learning, Convolutional Neural Networks}
}

% 定义中英文摘要和关键字
\begin{cabstract}
  
 近年来，卷积神经网络在计算机视觉、自然语言处理、语音识别等诸多领域均取得了突破性研究进展。特别是在视觉物体识别任务上，随着标注数据的积累和GPU计算性能的提升，卷积神经网络取得了越来越多的研究成果。本文针对视觉物体识别问题，研究了卷积神经网络在结构和方法上的改进。 取得的主要研究成果包括：
  
  \begin{itemize}
    \item 提出了卷积结构的一种改进：组合卷积结构/自适应卷积模块，用于简化复杂卷积神经网络的设计过程。通过对组合卷积模块的简单堆叠，来实现通用网络结构的设计。组合卷积结构结合了盗梦空间网络Inception、最大化输出单元Maxout、残差网络ResNet和网络中的网络NIN四个模型的优点，使得组合卷积结构具有更强的特征提取能力和更快的网络收敛速度。
    \item 提出了池化层的一种改进方法：随机区域池化，将数据增广的思想从数据层扩展到特征层，对卷积神经网络池化层特征进行增广。随机区域池化采用随机仿射变换对池化区域进行重采样，再对重采样后的区域进行池化操作。随机区域池化可以有效地增加特征的多样性，使网络对特征的扰动更加鲁棒，从而提高网络的泛化能力。
    \item 提出了一种基于主导卷积核分解和知识预回归的参数压缩与网络加速方法。主导卷积核分解是一种受矩阵低秩分解启发的卷积参数压缩方法，通过保留一个或多个具有主导作用的卷积核对卷积参数进行压缩。主导卷积核分解可以将卷积层的参数和计算量压缩到接近原来的12\%，极大地提高了模型的预测速度。为了弥补网络因参数压缩所造成的精度损失，受知识蒸馏方法启发，我们提出了知识预回归的网络训练方法，让压缩后的网络可以逐层地学习原网络的特征表达和泛化能力。
    \item 针对交通标志识别任务，我们将组合卷积结构和随机区域池化相结合，验证了两个方法的有效性。并采用主导卷积核分解对模型进行压缩，最后通过知识预回归训练方法提高模型的泛化能力，最终得到一个适用于交通标志识别的高精度快速卷积神经网络模型。
   \end{itemize}

 
  
 \end{cabstract}

% 如果习惯关键字跟在摘要文字后面，可以用直接命令来设置，如下：
% \ckeywords{\TeX, \LaTeX, CJK, 模板, 论文}

\begin{eabstract}

Convolutional neural networks play an increasingly important role in computer vision tasks, especially in the field of visual object recognition. In this dissertation, we study on the improvements of convolutional neural networks for visual object recognition. The innovative achievements for this dissertation include:

  \begin{itemize}
    \item We propose a novel module called self-adaptive module (SAM) to simplify design of any new deep learning architecture, since it no longer requires consideration of how to select receptive fields and depths. SAM is a combination of five prominent models, such as Inception, Maxout, ResNet, and NIN. It consists of four passes and one selector. Specifically, the four passes include two direct passes with different receptive fields and depths, one residual pass, and one Maxout pass. Actually, the residual pass is used to speed up convergence, while we take advantage of the Maxout pass to enhance approximate capabilities of SAM. The selector is further designed to help choose reasonable output.
    \item We propose a novel network model that incorporates a stochastic area pooling (SAP) method with a generic stacked T-shaped CNN architecture. In our SAP method, pooling area is randomly transformed and max pooling operation is then conducted on such areas, which means that regular pooling area of fixed upright squares are no longer exploited in the training phase of our SAPNet. In a sense, it could be viewed as the use of feature-level augmentation. Meanwhile, we present a generic CNN architecture that structurally resembles three stacked T-shaped cubes. In such architecture, the number of kernels in convolutional layer preceding any pooling layer is doubled to prevent the representational bottleneck problem.
    \item Aiming at accelerating the test time of deep convolutional neural networks, we propose a model compression method that contains a novel dominant kernel (DK) and a new training method called knowledge pre-regression (KP). DK is presented to significantly accomplish a low-rank decomposition of convolutional kernels, while KP is employed to transfer knowledge of intermediate hidden layers from a larger teacher network to its compressed student network on the basis of a cross entropy loss function instead of previous Euclidean distance.
  \end{itemize}

Finally, we demonstrate the effectiveness of our methods for traffic sign classification. On GTSRB dataset, we achieve a test accuracy of 99.66\%, and compress the number of parameters by a factor about 10. The compressed network is 10 times faster than before, and achieve a test accuracy of 99.27\%.


\end{eabstract}

% \ekeywords{\TeX, \LaTeX, CJK, template, thesis}
