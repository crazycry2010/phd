\chapter{总结与展望}
\label{cha:conclusion}

\section{总结}

卷积神经网络在计算机视觉领域，特别是视觉物体识别任务中取得了重大的研究突破，本文从卷积层、池化层和模型压缩与加速三个方面对卷积神经网络进行了改进，并针对交通标志识别问题对这三个方法进行了结合与验证。本文的主要研究内容按章节总结如下：

1. 提出了卷积结构的一种改进：组合卷积结构/自适应卷积模块，用于简化复杂卷积神经网络的设计过程。通过对组合卷积模块的简单堆叠，来实现通用网络结构的设计。组合卷积结构结合了盗梦空间网络Inception、最大化输出单元Maxout、残差网络ResNet和网络中的网络NIN四个模型的优点，使得组合卷积结构具有更强的特征提取能力和更快的网络收敛速度。

2. 提出了池化层的一种改进方法：随机区域池化，将数据增广的思想从数据层扩展到特征层，对卷积神经网络池化层特征进行增广。随机区域池化采用随机仿射变换对池化区域进行重采样，再对重采样后的区域进行池化操作。随机区域池化可以有效地增加特征的多样性，使网络对特征的扰动更加鲁棒，从而提高网络的泛化能力。

3. 提出了一种基于主导卷积核分解和知识预回归的参数压缩与网络加速方法。主导卷积核分解是一种受矩阵低秩分解启发的卷积参数压缩方法，通过保留一个或多个具有主导作用的卷积核对卷积参数进行压缩。主导卷积核分解可以将卷积层的参数和计算量压缩到接近原来的12\%，极大地提高了模型的预测速度。为了弥补网络因参数压缩所造成的精度损失，受知识蒸馏方法启发，我们提出了知识预回归的网络训练方法，让压缩后的网络可以逐层地学习原网络的特征表达和泛化能力。

4. 针对交通标志识别任务，我们将组合卷积结构和随机区域池化相结合，验证了两个方法的有效性。并采用主导卷积核分解对模型进行压缩，最后通过知识预回归训练方法提高模型的泛化能力，最终得到一个适用于交通标志识别的高精度快速卷积神经网络模型。

%第二章提出了一种卷积层的改进方法：具有组合卷积结构的自适应卷积模块（SAM）。组合卷积结构结合了Inception、Maxout、ResNet和NIN四个优秀网络模型的特点与优势。组合卷积结构以Inception结构为基础计算框架，由四条特征提取分支与一个特征选择器构成。Inception计算框架可以有效地平衡四条分支的参数规模和计算量，四条分支分别对应了不同的特征提取函数，其中包括两条卷积特征提取分支，分别具有不同的网络深度和感受野；一条Maxout分支可以增强组合卷积结构的非线性特征拟合能力；一条ResNet分支可以加快网络的收敛速度；特征选择器可以融合多通道的特征，同时控制特征的维度。Maxout与ResNet分支共享了两条卷积分支的参数，隐层特征的复用可以降低网络的参数规模和计算量，网络参数的复用可以有效避免过拟合现象。在CIFAR-10、CIFAR-100、MNIST和SVHN四个数据集上，我们对SAM进行了测试，分别取得了5.76\%，28.56\%，0.31\%和1.98\%的测试错误率。实验结果表明，使用SAM来搭建卷积神经网络，可以有效地简化网络的设计，同时保持卓越的网络泛化能力。

%第三章提出了一种池化层的改进方法：随机区域池化（SAP）。随机区域池化将数据增广的思想扩展到特征层，即特征增广。SAP通过随机仿射变换对池化区域进行重采样，在将传统的池化方法（最大池化或均值池化，本章采用的是最大池化）应用于重采样之后的池化区域。仿射变换后的区域往往处于非整数边界，因此采用双线性差值对非整数边界的像素进行估计。SAP在不改变特征空间样本分布的情况下，扩展特征空间，增加特征的多样性，从而提高网络的泛化能力，使网络对特征的扰动更加鲁棒。在CIFAR-10，CIFAR-100，MNIST和SVHN四个数据集上，我们对SAP进行了测试，分别取得了5.57\%，27.59\%，0.29\%和1.71\%的测试错误率。实验结果表明，SAP在没有明显增加网络参数与计算量的情况上，可以有效地提高网络的泛化能力。

%第四章提出了一种网络参数压缩与模型加速的方法：主导卷积核分解（DK分解）和知识预回归（KP训练）。DK分解是受矩阵低秩分解启发而提出的一种卷积参数压缩方法。DK分解将传统的卷积操作分解为特征提取和特征组合两个过程，在特征提取的过程中，对于每个输入特征图，我们仅选择具有主导作用的一个或多个卷积核对其进行特征提取。在特征组合的过程中，我们允许所有中间层特征参与特征的组合。DK分解可以将卷积层的参数和计算量压缩到原来的12\%左右，可以有效地压缩模型的参数，提高网络的预测速度。为了尽量弥补网络因压缩所造成的精度损失，我们提出了知识预回归的训练方法。将原网络（教师网络）部分隐层特征的泛化能力（知识）迁移到压缩后的网络（学生网络），让学生网络尽可能地去学习教师网络的特征表达能力和泛化能力。实验结果表明，知识预回归的训练方法可以加快网络的收敛速度，同时提高网络的泛化能力。

%第五章针对无人驾驶或高级辅助驾驶中交通标志识别问题，我们对前三章所提出的方法进行了对比分析。更进一步，我们将三个方法相结合，应用于解决交通标志识别问题。实验结果表明，组合卷积结构和随机区域池化可以有效地与其他卷积网络结构配合使用，提高网络的特征表达能力与网络的泛化能力。本章我们结合两种方法设计了一个具有特征增广的自适应网络模型FASNet，在德国交通标志识别数据集GTSRB上取得了99.66\%的识别率。针对FASNet网络参数和计算量较大的问题，我们采用主导卷积核分解对网络进行了压缩，得到压缩后的快速网络模型FastNet，并采用知识预回归的方法对FastNet进行训练，最终取得了99.27\%的识别率。相对于原始的FASNet网络结构，FastNet具有近10倍的参数压缩和预测速度提升。


\section{展望}

本文从卷积层、池化层、网络参数压缩和训练方法三个方向对卷积神经网络进行了创新，在视觉物体识别任务上取得了一些研究进展，但是在这些研究方向上，仍存在进一步研究与改进的空间。

首先，对于组合卷积结构，目前仅仅在SAM模块中引入了四条特征提取分支。随着卷积神经网络的飞速发展，优秀的网络模型会越来越多，将更多的模型组合到SAM是一个长期积累的过程。将这些优秀结构有效地组合与应用，进一步简化复杂卷积神经网络的设计过程，使卷积神经网络得到更加广泛的应用。此外SAM本身的结构相对较为复杂，导致对于SAM的理解和可视化工作变得非常困难，而网络的可视化工作是理解和优化网络结构的有效手段。

其次，对于特征增广，这是一个提高网络泛化能力的简单有效方法，目前我们仅仅在池化层验证了特征增广的效果。对于卷积神经网络来说，它的一个主要的特点就是分层的特征表达，如何将特征增广应用于更多隐层是一个值得研究的课题。此外对于随机区域池化方法本身，我们仅仅研究了具有旋转、平移和缩放的特征增广方式，我们相信诸如错切、翻转甚至特征空间变换等其他特征增广方式可以进一步提高网络的泛化能力。

最后，对于网络加速，这是卷积神经网络从研究走入实际应用的一个必要环节。在主导卷积核分解的方法中，我们仅仅对卷积层进行了参数压缩，这是因为在我们的应用场景中卷积层占据了几乎全部的参数。而对于其他分类任务，比如百万级别、甚至上亿级别的人脸分类问题，全连接层也会引入大量的参数，如何有效地对全连接层进行DK分解，是一个有有待研究的课题。基于矩阵低秩分解的压缩方法毕竟还是精度有损的压缩，尽管知识预回归弥补了一部分精度损失，但是精度无损的压缩方法仍然是一个有趣的研究课题。


